{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b3d24d",
   "metadata": {},
   "source": [
    "# A1.2 Feed forward network\n",
    "\n",
    "In this part of the assignment we will develop our own building blocks for constructing a feed forward network.\n",
    "We will follow a modular approach so that we can use these building blocks in feed forward architecture of our choice.\n",
    "\n",
    "We will follow the logic of computation graphs where the layers and the loss have the characteristics of the compute nodes in terms of locality and ability to communicate with upstream and downstream blocks.\n",
    "\n",
    "Instead of defining the forward and backward steps as functions that need to pass around cached variables, we will implement the compute nodes as statefull objects - instantiations of python classes with forward and backward methods.\n",
    "\n",
    "We will then conscruct a 2 layer neural network and use our newly developed functionality to predict the target values and compute the parameter gradients.\n",
    "\n",
    "Work through the cells below and complete the tasks indicated by <span style=\"color:red\">**TODO**</span> here below and in the script `ann_code/layers.py` (replace `pass` with the appropriate code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d6f0ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# necessary initialization\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "979a3fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 90, input dimensions: 3.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from ann_code.helpers import load_data\n",
    "in_data, labels = load_data(filename='./ann_data/toy_data.csv') # correct filename if necessary\n",
    "\n",
    "# get data dimensions\n",
    "num_inst, num_dim = in_data.shape\n",
    "print(f\"Number of instances: {num_inst}, input dimensions: {num_dim}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dd5a4",
   "metadata": {},
   "source": [
    "## 1) Forward pass\n",
    "\n",
    "We first work on the forward pass functionality of our layer objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798266a9",
   "metadata": {},
   "source": [
    "### Linear layer\n",
    "\n",
    "We start by defyining the linear layer.\n",
    "Complete the `__init__` and `forward` methods of the `Linear` class in `ann_code/layers.py`.\n",
    "\n",
    "The class object instances shall be initialized with the linear function parameters (weight and bias) as the instance attributes.\n",
    "The other local information (inputs, outputs and their gradients) shall be also defined as the instance object attributes and will be populated by the `forward` and `backward` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e65e8e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your outputs tensor([[ 1.0220,  1.0258,  1.0295,  1.0329,  1.0361,  1.0391,  1.0418,  1.0441,\n",
      "          1.0462,  1.0479],\n",
      "        [-0.4527, -0.5533, -0.6615, -0.7779, -0.9030, -1.0374, -1.1819, -1.3370,\n",
      "         -1.5037, -1.6827]])\n"
     ]
    }
   ],
   "source": [
    "################################### ERROR\n",
    "\n",
    "\n",
    "# after implementing Linear class, check it here\n",
    "from ann_code.layers import Linear\n",
    "\n",
    "# initiate w and b buffers\n",
    "# we use these for initiating the model parameters instead of the usual random init\n",
    "# this is to make sure that yours and mine results match\n",
    "w_buffer = torch.logspace(start=0.1, end=10, steps=1000)\n",
    "b_buffer = torch.logspace(start=0.1, end=10, steps=1000, base=2)\n",
    "\n",
    "# linear layer dimensions\n",
    "in_features = num_dim\n",
    "out_features = 10\n",
    "################################################################################\n",
    "### START OF YOUR CODE                                                         #\n",
    "### TODO: initiate a linear layer instance                                     #\n",
    "################################################################################\n",
    "# initialize linear layer parameters from the buffers\n",
    "# first extract from the buffers the necessary number of elements \n",
    "# followed by view() to get the correct shape\n",
    "# e.g. for 2x3 w matrix with 6 elements in total do \n",
    "# w = w_buffer[:6].view(2, 3)\n",
    "# pass\n",
    "\n",
    "# Extract the necessary number of elements for w and b and reshape them\n",
    "w_elements = in_features * out_features\n",
    "b_elements = out_features\n",
    "\n",
    "w = w_buffer[:w_elements].view(out_features, in_features)\n",
    "b = b_buffer[:b_elements].view(out_features)\n",
    "# instantiate the linear layer objectygbb                                                                                                                                                                                      \n",
    "# Instantiate the Linear layer object\n",
    "linear = Linear(W=w, b=b)\n",
    "# linear.weight = torch.nn.Parameter(w)  # Set weight as a Parameter\n",
    "# linear.bias = torch.nn.Parameter(b)\n",
    "# forward pass in_data through the layer\n",
    "outputs = linear.forward(in_data)\n",
    "batch_size = 2\n",
    "################################################################################\n",
    "### END OF YOUR CODE                                                           #\n",
    "################################################################################\n",
    "\n",
    "# forward pass in_data througah the layer\n",
    "outputs = linear.forward(in_data)\n",
    "\n",
    "# check outputs for the first two data instances\n",
    "print(f'Your outputs {outputs[:2,:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d9d18",
   "metadata": {},
   "source": [
    "Expected outputs\n",
    "\n",
    "`tensor([[ 1.0220,  1.0258,  1.0295,  1.0329,  1.0361,  1.0391,  1.0418,  1.0441,\n",
    "          1.0462,  1.0479],\n",
    "        [-0.4527, -0.5533, -0.6615, -0.7779, -0.9030, -1.0374, -1.1819, -1.3370,\n",
    "         -1.5037, -1.6827]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1e04c",
   "metadata": {},
   "source": [
    "### ReLU nonlinearity\n",
    "\n",
    "We next defined the class for the Rectified Linear Unit which is an element-wise operation defined as $ReLU(x) = max(0, x).$\n",
    "\n",
    "Complete the `forward` methods of the `Relu` class in `ann_code/layers.py`. Note that in this case, there are no parameters that should be included in the object instances as initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c8941aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your outputs tensor([[0.8872, 0.0000, 0.3707],\n",
      "        [0.0000, 1.3094, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# After implementing Relu class, check it here\n",
    "from ann_code.layers import Relu\n",
    "\n",
    "# relu instance\n",
    "relu = Relu()\n",
    "\n",
    "# forward pass in_data through the layer\n",
    "outputs = relu.forward(in_data)\n",
    "\n",
    "# check outputs for the first two data instances\n",
    "print(f'Your outputs {outputs[:2,:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278bafde",
   "metadata": {},
   "source": [
    "Expected outputs\n",
    "\n",
    "`tensor([[0.8872, 0.0000, 0.3707],\n",
    "        [0.0000, 1.3094, 0.0000]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c79b9",
   "metadata": {},
   "source": [
    "### Define network with on hidden layer\n",
    "\n",
    "We use the linear and relu classes to create a network with the following architecture. \n",
    "We combine the layers through the `Model` class that I defined for you in the `ann_code/layers.py`\n",
    "\n",
    "We will add the MSE less in a later step, now do just the forward pass through the layers to obtain the predicitons.\n",
    "\n",
    "<center><img src=\"net_diagram.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ad5a04d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (90x2 and 3x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# forward pass in_data through all layers to get predictions\u001b[39;00m\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(layers)\n\u001b[1;32m---> 30\u001b[0m ypred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# check outputs for the first two data instances\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYour outputs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mypred[:\u001b[38;5;241m2\u001b[39m,:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\SS24_Assignment1-main\\SS24_Assignment1-main\\ann_code\\layers.py:153\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, ins)\u001b[0m\n\u001b[0;32m    151\u001b[0m outs \u001b[38;5;241m=\u001b[39m ins\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 153\u001b[0m \touts \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\SS24_Assignment1-main\\SS24_Assignment1-main\\ann_code\\layers.py:58\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, ins)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;66;03m# Perform linear transformation: X * W^T + b\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mins \u001b[38;5;241m=\u001b[39m ins\n\u001b[1;32m---> 58\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouts \u001b[38;5;241m=\u001b[39m \u001b[43mins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[0;32m     60\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouts\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (90x2 and 3x3)"
     ]
    }
   ],
   "source": [
    "# work with Model class to do the forward pass through the network\n",
    "from ann_code.layers import Model\n",
    "\n",
    "################################################################################\n",
    "### START OF YOUR CODE                                                         #\n",
    "### TODO: initiate all layers of nerual network                                #\n",
    "################################################################################\n",
    "# initialize parameters for all layers from the w_buffer and b_buffer\n",
    "# first extract from the buffers the necessary number of elements \n",
    "# followed by view() to get the correct shape\n",
    "# start after the last previously used element of the buffer for every new parameter from the same buffer\n",
    "# e.g. for 2x3 matrix w1 with 6 elements and 3x3 matrix w2 with 9 elements do\n",
    "\n",
    "# Initialize parameters for all layers from the w_buffer and b_buffer\n",
    "w1 = w_buffer[:6].view(2, 3)    # Assuming the first layer has 2 output features and 3 input features\n",
    "b1 = b_buffer[:2].view(2)       # Corresponding bias for the first layer with 2 output features\n",
    "w2 = w_buffer[6:15].view(3, 3)  # Assuming the second layer has 3 output features and 3 input features\n",
    "b2 = b_buffer[2:5].view(3)      # Corresponding bias for the second layer with 3 output features\n",
    "\n",
    "# Define all necessary layers as instances of the Linear and Relu classes\n",
    "layer1 = Linear(w1, b1)\n",
    "relu = Relu()\n",
    "layer2 = Linear(w2, b2)\n",
    "\n",
    "# Create a list with the layers to be passed to Model(layers)\n",
    "layers = [layer1, relu, layer2]\n",
    "\n",
    "# forward pass in_data through all layers to get predictions\n",
    "model = Model(layers)\n",
    "ypred = model.forward(in_data)\n",
    "\n",
    "\n",
    "# check outputs for the first two data instances\n",
    "print(f'Your outputs {ypred[:2,:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87699ab",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "`tensor([[8.1458],\n",
    "        [1.1016]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f299c",
   "metadata": {},
   "source": [
    "## 3) MSE loss\n",
    "\n",
    "We use the MSE loss functions defined in `ann_code/linear_regression.py` to get the mse loss for our predictions and the corresponding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f704f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mse functions defined for linear regression to get the MSE and gradient with respect to predictions\n",
    "from ann_code.linear_regression import mse_forward, mse_backward\n",
    "\n",
    "loss, mse_cache = mse_forward(ypred, labels)\n",
    "ypredgrad, _ = mse_backward(mse_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3bcb0",
   "metadata": {},
   "source": [
    "## 3) Backward propagation\n",
    "\n",
    "Finally, you need to implement the `backward` methods in for the `Linear` and `Relu` classes.\n",
    "\n",
    "Remember that you need to use the chain rule and combine the local and the upstream gradient to obtain the global gradients. Do not forget that ReLu is an element-wise operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4ced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global gradient of loss with respect to weight parameters tensor([[106.2968, 108.7577, 111.4530, 114.4143]])\n",
      "Global gradient of loss with respect to bias parameters tensor([[90.]])\n",
      "Global gradient of loss with respect to linear layer inputs tensor([[1.6555, 1.6937, 1.7328, 1.7728],\n",
      "        [1.6555, 1.6937, 1.7328, 1.7728]])\n"
     ]
    }
   ],
   "source": [
    "# After implementing the backward passes of Linear class test it here\n",
    "\n",
    "# do the backward pass of last linear layer\n",
    "lin2.backward(torch.ones(num_inst, 1))\n",
    "\n",
    "# check global gradients\n",
    "print(f'Global gradient of loss with respect to weight parameters {lin2.W.g}')\n",
    "print(f'Global gradient of loss with respect to bias parameters {lin2.b.g}')\n",
    "print(f'Global gradient of loss with respect to linear layer inputs {lin2.ins.g[:2,:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853b139",
   "metadata": {},
   "source": [
    "Expected results\n",
    "\n",
    "`Global gradient of loss with respect to weight parameters tensor([[106.2968, 108.7577, 111.4530, 114.4143]])`\n",
    "\n",
    "`Global gradient of loss with respect to bias parameters tensor([[90.]])`\n",
    "\n",
    "`Global gradient of loss with respect to linear layer inputs tensor([[1.6555, 1.6937, 1.7328, 1.7728],\n",
    "        [1.6555, 1.6937, 1.7328, 1.7728]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a043849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global gradient of loss with respect to relu inputs tensor([[0., 1., 2., 3.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# After implementing the backward passes of relu class test it here\n",
    "\n",
    "# do the backward pass of relu\n",
    "relu1.backward(torch.arange(num_inst*4).view(num_inst, 4))\n",
    "\n",
    "# check global gradients\n",
    "print(f'Global gradient of loss with respect to relu inputs {relu1.ins.g[:2,:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0ffd7",
   "metadata": {},
   "source": [
    "Expected results\n",
    "\n",
    "`Global gradient of loss with respect to relu inputs tensor([[0., 1., 2., 3.],\n",
    "        [0., 0., 0., 0.]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17daa25f",
   "metadata": {},
   "source": [
    "## Complete backward pass\n",
    "\n",
    "We shall use the Model class to get the gradients of all the layers and their parameters with respect to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8234339",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ypredgrad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mann_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m grad_model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# do the backward pass through the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mbackward(\u001b[43mypredgrad\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print out your gradients of loss with respect to the parameters of the 1st model layer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYour dLoss/dW1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ypredgrad' is not defined"
     ]
    }
   ],
   "source": [
    "from ann_code.helpers import grad_model\n",
    "\n",
    "# do the backward pass through the model\n",
    "model.backward(ypredgrad)\n",
    "\n",
    "# print out your gradients of loss with respect to the parameters of the 1st model layer\n",
    "print(f'Your dLoss/dW1: {model.layers[0].W.g}')\n",
    "print(f'Your dLoss/db1: {model.layers[0].b.g}')\n",
    "print(f'Your dLoss/dins: {model.layers[0].ins.g[:2, :]}')\n",
    "\n",
    "# print out correct gradients of loss with respect to the parameters of the 1st model layer\n",
    "# these should be the same as your gradients from above\n",
    "model_check = grad_model(model, in_data, labels)\n",
    "print(f'Correct dLoss/dW1: {model_check.layers[0].W.grad}')\n",
    "print(f'Correct dLoss/db1: {model_check.layers[0].b.grad}')\n",
    "print(f'Correct dLoss/dins: {model_check.layers[0].ins.grad[:2, :]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57647f",
   "metadata": {},
   "source": [
    "## 4) Multilayer feed forward network\n",
    "\n",
    "Finally, use your `Linear` and `Relu` classes and combine them with the `Model` class to construct a more complicated network.\n",
    "\n",
    "Define a network with the following architecture:\n",
    "Linear: input_dim = 3, output_dim = 5 -> Relu ->\n",
    "Linear: input_dim = 5, output_dim = 10 -> Relu ->\n",
    "Linear: input_dim = 10, output_dim = 4 -> Relu ->\n",
    "Linear: input_dim = 4, output_dim = 1\n",
    "\n",
    "Initialize all the linear layers with parameters W and b sampled randomly from standardat normal distribution.\n",
    "\n",
    "Combine the layers using the `Model` class and get the predictions (`forward` method).\n",
    "\n",
    "Use the MSE forward and backward functions to get the loss and the gradient with respect to the predictions.\n",
    "\n",
    "Use the `backward` method of `Model` to get all the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78711afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "### START OF YOUR CODE                                                         #\n",
    "### TODO: define mffn as instance of Model class                               #\n",
    "################################################################################\n",
    "\n",
    "# instantiate all layers\n",
    "pass\n",
    "\n",
    "# define model using Model class\n",
    "mffn = Model(layers)\n",
    "\n",
    "# forward, mse, backward\n",
    "pass\n",
    "\n",
    "################################################################################\n",
    "### END OF YOUR CODE                                                           #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e16779",
   "metadata": {},
   "source": [
    "#### Check model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your NN architecture definitions seems CORRECT.\n"
     ]
    }
   ],
   "source": [
    "# check architecture\n",
    "from ann_code.helpers import check_architecture\n",
    "\n",
    "check_architecture(mffn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88eecbb",
   "metadata": {},
   "source": [
    "#### Check gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dLoss/dW1: tensor([[-0.0642,  0.3048, -0.6759],\n",
      "        [ 0.0927, -0.1344,  0.1958],\n",
      "        [-0.7675,  1.4048, -1.7181],\n",
      "        [-0.0871,  0.5533, -0.8004],\n",
      "        [-0.0665, -0.2602,  0.2186]], grad_fn=<MmBackward0>)\n",
      "Your dLoss/db1: tensor([[ 0.5142, -0.1490,  1.3070,  0.2416,  0.3607]], grad_fn=<SumBackward1>)\n",
      "Your dLoss/dins: tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0597, -0.0060, -0.2855]], grad_fn=<SliceBackward0>)\n",
      "Correct dLoss/dW1: tensor([[-0.0642,  0.3048, -0.6759],\n",
      "        [ 0.0927, -0.1344,  0.1958],\n",
      "        [-0.7675,  1.4048, -1.7181],\n",
      "        [-0.0871,  0.5533, -0.8004],\n",
      "        [-0.0665, -0.2602,  0.2186]])\n",
      "Correct dLoss/db1: tensor([[ 0.5142, -0.1490,  1.3070,  0.2416,  0.3607]])\n",
      "Correct dLoss/dins: tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0597, -0.0060, -0.2855]])\n"
     ]
    }
   ],
   "source": [
    "# print out your gradients of loss with respect to the parameters of the 1st model layer\n",
    "print(f'Your dLoss/dW1: {mffn.layers[0].W.g}')\n",
    "print(f'Your dLoss/db1: {mffn.layers[0].b.g}')\n",
    "print(f'Your dLoss/dins: {mffn.layers[0].ins.g[:2, :]}') \n",
    "    \n",
    "# print out correct gradients of loss with respect to the parameters of the 1st model layer\n",
    "# these should be the same as your gradients from above\n",
    "model_check = grad_model(mffn, in_data, labels)\n",
    "print(f'Correct dLoss/dW1: {model_check.layers[0].W.grad}')\n",
    "print(f'Correct dLoss/db1: {model_check.layers[0].b.grad}')\n",
    "print(f'Correct dLoss/dins: {model_check.layers[0].ins.grad[:2, :]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
